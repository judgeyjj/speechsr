Versatile audio super-resolution (SR) aims to predict high-
frequency components from low-resolution audio across di-
verse domains such as speech, music, and sound effects. Ex-
isting diffusion-based SR methods often fail to produce se-
mantically aligned outputs and struggle with consistent high-
frequency reconstruction. In this paper, we propose SAGA-
SR, a versatile audio SR model that combines semantic and
acoustic guidance. Based on a DiT backbone trained with
a flow matching objective, SAGA-SR is conditioned on text
and spectral roll-off embeddings. Due to the effective guid-
ance provided by its conditioning, SAGA-SR robustly upsam-
ples audio from arbitrary input sampling rates between 4 kHz
and 32 kHz to 44.1 kHz. Both objective and subjective eval-
uations show that SAGA-SR achieves state-of-the-art perfor-
mance across all test cases. Sound examples and code for the
proposed model are available online1
.
Index Terms— audio super-resolution, bandwidth exten-
sion, flow matching, generative model
1. INTRODUCTION
Audio super-resolution (SR) aims to reconstruct a high-
resolution audio signal from its corresponding low-resolution
audio signal. To enhance listening experiences, it can be ap-
plied to diverse audio types, including historical recordings,
low-bandwidth telephone audio, and audio generated by deep
learning models [1]. Previous audio SR methods [2, 3] based
on deep neural networks have achieved promising perfor-
mance in constrained settings, including fixed upsampling
ratios and restricted domains such as speech or music. How-
ever, these constraints limit their applicability in diverse and
complex real-world scenarios.
To address this issue, several recent works [1, 4] have fo-
cused on versatile audio super-resolution, which is the task of
upsampling general-domain audio, including music, speech,
and sound effects, from varying input sampling rates to full
bandwidth, such as 44.1 kHz or 48 kHz. AudioSR [1] em-
ploys a latent diffusion model (LDM) [5] with a Transformer-
UNet backbone [6] to capture the complex distributions of
general audio signals. FlashSR [4] employs diffusion distil-
lation [7] to train the Student LDM using AudioSR as the
Teacher LDM and proposes the SR Vocoder to further en-
hance AudioSR’s performance.
Although previous methods have achieved notable suc-
cess, there is still room for improvement. There are two key
challenges hindering the performance of versatile audio SR
models. First, to generate natural high-resolution audio, an
audio SR model needs to capture semantic information from
low-resolution input and effectively incorporate it into the
reconstruction process. Previous methods often fail to predict
semantically-aligned high-frequency components, resulting
in unnatural artifacts, such as excessive sibilance [4]. Second,
unlike speech SR, versatile audio SR handles a much broader
range of audio domains, which exhibit high diversity in high-
frequency energy distributions. This results in difficulties for
models in consistently reconstructing high-frequency content,
especially when the input has a low cutoff-frequency (e.g., 4
kHz).
In this paper, we present SAGA-SR, a versatile audio
super-resolution model that leverages semantic and acous-
tic conditions. SAGA-SR is based on a DiT [8] backbone
trained with a flow matching objective [9], incorporating
two key conditions. First, inspired by recent works [10, 11]
in the computer vision domain, we utilize text embeddings
for semantic guidance. Specifically, we employ an audio-
language model to generate text captions from audio, en-
abling more efficient training and inference. Second, we
introduce spectral roll-off embeddings, which provide rela-
tive high-frequency energy information for both the input and
target audio. Guided by both semantic and acoustic condi-
tions, SAGA-SR can robustly upsample music, speech, and
sound effects from any sampling rate between 4 kHz and 32
kHz to 44.1 kHz, and achieves state-of-the-art performance
on both objective and subjective evaluations.
This work was supported by the National Research Foundation of Ko-
rea (NRF) grant funded by the Korea government (MSIT) (No. RS-2023-
00222383).
1http://jakeoneijk.github.io/saga-sr-project
2. METHOD
Figure 1 shows the overall architecture of SAGA-SR. Let xh,
xl ∈R2×L represent the high-resolution and low-resolution
audio, respectively, where Lis the number of samples. Each
audio sample is compressed into latent representations zh ∈
R64×L/2048 and zl ∈R64×L/2048 by the pre-trained VAE en-
coder from [12]. The text c describes the audio content in-
Fig. 1. Overview of SAGA-SR
dependent of its resolution. Roll-off frequencies fh, fl ∈R
are extracted from xh and xl, respectively. DiT estimates zh
from zl, c, fh, and fl. The predicted latent is then converted
into an audio signal by the pre-trained VAE decoder, followed
by low-frequency replacement post-processing [1] to ensure
consistency in low-frequency information. In Section 2.1, we
present the training and inference procedures of DiT. In Sec-
tion 2.2, we describe how each condition is processed to be
incorporated into DiT.
2.1. DiT Model
Unlike previous works [1, 4] that employ a Transformer-UNet
architecture, we use DiT [8], which is widely adopted in im-
age and audio generation models. We adapt the DiT archi-
tecture proposed in [12] and train it using the conditional
flow matching objective [9]. The flow matching objective re-
gresses onto a target vector field that generates a probability
path, transforming a simple distribution into an approxima-
tion of the data distribution. We use a linear interpolation
path between the noise and the data as follows:
zt = (1−t)·z0 + t·z1, (1)
where z1 = zh, z0 ∼N(0,1), and t ∈[0,1]. The corre-
sponding velocity at zt is given by
dzt
vt =
dt= z1−z0. (2)
The training objective for DiT is defined as
Et,z0 ,z1 ,zl ,c∥u(zt,zl,c,fh,fl,t; θ)−vt∥2
, (3)
where θdenotes the model parameters. To condition DiT on
zl, we concatenate zl with zt along the channel dimension. A
dropout rate of 10% is applied to zl, enabling classifier-free
guidance [13].
At inference, we sample z0 ∼N(0,1) and use an ODE
solver to generateˆ
zh. In practice, we adopt the Euler sam-
pler with a linear-quadratic t-schedule [14] and 100 inference
steps. To control the influence of zl and c, we adopt the
classifier-free guidance method introduced in [15].
uCFG(zt,zl,c,fh,fl,t; θ) = u(zt,∅,∅,fh,fl,t; θ)
+ sa(u(zt,zl,∅,fh,fl,t; θ)−u(zt,∅,∅,fh,fl,t; θ))
+ st(u(zt,zl,c,fh,fl,t; θ)−u(zt,zl,∅,fh,fl,t; θ)),
(4)
where sa and st are guidance scales, and ∅ denotes null con-
ditioning. We empirically set sa = 1.4 and st = 1.2.
2.2. Conditioning
Text embedding. Audio super-resolution models are typ-
ically trained with high-resolution audio-only data, since
low-resolution audio can be simulated from it. Compared
to audio-only data, audio-text data is expensive to curate
at scale. Furthermore, relying on user-provided text during
inference can limit practical applicability and potentially de-
grade generation quality. To address these issues, we employ
Qwen2-Audio [16] to generate text captions from audio. Dur-
ing training, captions generated from high-resolution audio
are used for efficiency, while at inference, they are derived
from low-resolution audio. Text embeddings are extracted
from the generated captions using a pretrained T5-base en-
coder [17] and provided to the DiT through cross-attention.
A dropout rate of 10% is applied to the text embeddings.
Spectral roll-off embedding. We compute the roll-off fre-
quency from the STFT spectrogram using an open-source
method2. Instead of computing it frame-wise as in the orig-
inal implementation, we sum over the time axis of the mag-
nitude spectrogram to obtain a single roll-off frequency value
for each audio sample, which is then normalized to [0,1)
using the min–max normalization. The normalized roll-off
frequency value is projected into learnable Fourier embed-
dings. We extract the spectral roll-off embeddings from both
low-resolution and high-resolution audio.
The spectral roll-off embeddings are conditioned into the
DiT through two mechanisms. First, they are concatenated
with the text embeddings along the sequence dimension be-
fore cross-attention. Second, the input and target roll-off em-
beddings are concatenated along the channel dimension, pro-
jected by linear layers, summed with the timestep sinusoidal
embeddings [18], and then prepended to the input of DiT. The
input roll-off embeddings provide the DiT with information
about the cutoff frequency of the input audio, improving its
2https://librosa.org/doc/0.11.0/generated/
librosa.feature.spectral_rolloff.html
ability to handle varying input sampling rates, while the tar-
get roll-off embeddings guide the amount of high-frequency
energy to generate. During inference, the target normalized
roll-off frequency serves as conditioning, enabling the user to
control the high-frequency energy in the generated audio. Be-
cause it is represented as a single scalar in [0,1), it is straight-
forward to manipulate.
3. EXPERIMENTS
3.1. Training Dataset and Preprocessing
Our training dataset configuration and data simulation method
are consistent with previous works [1, 4]. We train on the
FreeSound [19]3, MedleyDB [20], MUSDB18-HQ [21], Moi-
sesDB [22], and OpenSLR4 speech dataset [23], with a total
audio duration of around 3,800 hours. All audio was resam-
pled to 44.1 kHz and randomly segmented into 5.94-second
clips for training. To simulate low-high resolution audio pairs,
we apply low-pass filtering to the high-resolution audio. The
cutoff frequency is uniformly sampled between 2 kHz and
16 kHz. The low-pass filter type is randomly selected from
Chebyshev, Butterworth, Bessel, and Elliptic, with the filter
order chosen between 2 and 10.
3.2. Implementation Details
The DiT was trained for 26,000 steps using the AdamW opti-
mizer with β1 = 0.9 and β2 = 0.999. We use a batch size of
256 and a learning rate of 1.0 ×10−5. An InverseLR sched-
uler [12] is applied with an inverse gamma of 106, a power
of 0.5, and a warmup factor of 0.99. To compute the roll-
off frequency, we extract the STFT spectrogram using a Hann
window of 2048 and a hop size of 512. The roll-off percent-
age is set to 0.985.
3.3. Evaluation
Comparison. We compare SAGA-SR against state-of-the-art
models, AudioSR [1] and FlashSR [4]. The official imple-
mentations and checkpoints are used for all comparison mod-
els. In addition, we conducted an ablation study to evaluate
the effectiveness of the text embedding and the spectral roll-
off embedding. We train two SAGA-SR variants, one without
the text embedding and another without the spectral roll-off
embedding. Both models are trained under the same settings
as SAGA-SR.
Dataset. For both objective and subjective evaluations, we
adopt the VCTK test set (speech) [24], FMA-small (music)
[25], and ESC50 fold-5 (sound effects) [26]. We selected 400
samples from each dataset.
Evaluation Metrics. For objective evaluation, each sound
category is evaluated at cutoff frequencies of 4 kHz and 8
3https://labs.freesound.org/
4https://openslr.org/
kHz. Log-Spectral Distance (LSD) is used as the evaluation
metric, following previous studies [1, 4, 24]. Although the
LSD metric is widely used in audio super-resolution tasks, it
has been found that it does not always align with perceptual
quality [1, 4]. To complement LSD, we also adopt the Fr´ echet
Distance (FD) based on OpenL3 [27] for evaluating the music
and sound effects categories. FD compares the statistics of
embeddings from generated audio with those from ground-
truth audio.
For subjective evaluation, a listening test was conducted
with 25 participants. For all sound categories, the cutoff fre-
quency was set to 4 kHz. During the test, participants were
provided with the low-resolution input audio as a low anchor
and asked to rate the perceptual quality of the outputs from
each model on a scale from 1 to 5. We evaluated AudioSR,
FlashSR, and the proposed SAGA-SR.
4. RESULTS
4.1. Objective Evaluation
Table 1 shows the results of the objective evaluation. SAGA-
SR achieves state-of-the-art performance across all metrics
and test cases, demonstrating the effectiveness of the pro-
posed method. Compared to its variant without the spectral
roll-off embedding, SAGA-SR consistently achieves better
performance across all metrics and test cases. This indicates
that the spectral roll-off embedding enhances the model’s
ability to handle varying cutoff frequencies and guides it
to generate outputs with the desired high-frequency energy.
When compared with its variant without the text embedding,
SAGA-SR achieves superior performance in the speech SR
task. In the music and sound effect SR tasks, SAGA-SR
and its variant achieve comparable performance in terms of
LSD. On the other hand, SAGA-SR consistently outperforms
its variant in FD. These results demonstrate that, while the
spectral roll-off embedding improves spectral alignment with
ground-truth audio, the text embedding is essential for gener-
ating outputs that are more plausible and perceptually aligned
with the reference audio.
4.2. Subjective Evaluation
Table 2 presents the results of the subjective evaluation.
SAGA-SR achieves the highest scores across all test cases.
Figure 2 shows a comparison of the STFT spectrograms for
audio generated by different models. We found that AudioSR
and FlashSR exhibit high variance in their outputs and often
produce audio lacking sufficient high-frequency content. In
contrast, SAGA-SR demonstrates better consistency in re-
constructing high-frequency components, due to the explicit
guidance provided by the spectral roll-off embedding. More-
over, we observed that AudioSR and FlashSR often generate
audio that is not semantically aligned with the low-resolution
input audio. Specifically, AudioSR tends to produce outputs
Table 1. Objective evaluation results. Bold numbers indicate the best performance, while underlined numbers denote the
second-best performance across all models. Note that low-frequency replacement post-processing was applied to the audio
reconstructed by the VAE.
Speech Music Sound Effect
Method 4kHz 8kHz LSD ↓ LSD ↓ 4kHz 8kHz LSD ↓ FD ↓ LSD ↓ FD ↓ 4kHz 8kHz
LSD ↓ FD ↓ LSD ↓ FD ↓
Unprocessed VAE (recon) 2.89 2.49 0.87 0.81 3.68 138.09 2.68 106.46 1.13 18.92 1.06 17.30 3.30 110.25 2.39 64.08
1.13 13.47 1.08 11.53
AudioSR [1] FlashSR [4] 1.46 1.26 1.47 1.15 2.09 32.52 1.88 25.93 1.76 37.79 1.69 32.08 1.85 39.69 1.68 28.54
1.81 41.32 1.89 36.13
SAGA-SR w/o text w/o roll-off 1.28 1.07 1.32 1.11 1.64 23.87 1.45 20.44 1.65 26.32 1.43 21.86
1.63 30.14 1.45 25.34 1.60 29.00 1.43 23.94
1.57 1.43 2.16 35.99 1.78 23.5 2.19 33.07 1.75 22.4
Fig. 2. Spectrogram of compared models.
Table 2. Subjective evaluation results.
Method Speech Music Sound Effect
Unprocessed Ground Truth 1.81 4.23 1.66 3.93 1.77
4.18
AudioSR [1] FlashSR [4] SAGA-SR 3.26 3.45 3.70 2.94 3.46 3.65 3.03
3.34
3.88
5. CONCLUSIONS
Fig. 3. Generated samples with varying scales of the target
normalized roll-off frequency
with excessive sibilance, while FlashSR often fails to gener-
ate detailed harmonic structures, as illustrated in Figure 2. By
incorporating text embeddings, SAGA-SR is able to generate
semantically aligned audio with realistic harmonic detail.
As shown in Figure 3, SAGA-SR allows the user to con-
trol the high-frequency energy of the generated audio by ad-
justing the target normalized roll-off frequency, a single scalar
condition. This control not only reduces the variance in per-
ceptual quality but also influences acoustic properties such as
timbre.
We propose SAGA-SR, a versatile audio super-resolution
model that integrates semantic and acoustic conditioning
into a DiT backbone trained with a flow matching objective.
Text embeddings improve semantic alignment, while spectral
roll-off embeddings enhance robustness and controllability
in high-frequency reconstruction. Both objective and subjec-
tive evaluations show that SAGA-SR outperforms previous
methods across all tasks and metrics.
Despite its effectiveness, SAGA-SR has limitations that
warrant future work. First, upsampling audio with multiple
overlapping sources remains challenging. Future work could
scale data and model capacity or develop improved caption-
ing methods to accurately describe all sound sources. Sec-
ond, low-frequency replacement post-processing may intro-
duce unnatural connections between high and low frequency
components. A potential direction is to develop a VAE con-
ditioned on the low-resolution waveform to achieve smoother
integration