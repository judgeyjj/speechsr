# 基于 SAGA-SR 框架实现语音超分的技术路线

本文档旨在阐述如何**借鉴并改造** SAGA-SR 这一先进的通用音频超分框架，以实现一个高性能的**语音超分**模型。核心思路是保留其强大的 VAE + DiT 生成主干，同时对其引导机制进行特化，注入语音领域独有的先验知识。

---

### **第一部分：理解作为基础的 SAGA-SR 框架**

SAGA-SR 是一个强大的通用音频超分模型，其核心组件为我们提供了坚实的基础。

1.  **核心架构: VAE + DiT**
    *   **预训练 VAE**: 采用 Stable Audio 提供的预训练 VAE，负责将复杂的音频波形与低维的潜在表示 (Latent) 进行高效转换。
    *   **DiT 骨干网络**: 使用 Diffusion Transformer 作为去噪网络的主体，在潜在空间中学习从噪声和低分辨率音频恢复高分辨率音频的映射关系。

2.  **SAGA-SR 的原始引导机制**
    *   **语义引导 (Semantic Guidance)**: 针对**通用音频**，使用 `Qwen-Audio` 等模型生成描述性文本 (Audio Captioning)，再通过 `T5-base` 编码器转换为 embedding，为 DiT 提供内容先验。
    *   **声学引导 (Acoustic Guidance)**: 计算音频的**频谱滚降点 (Spectral Roll-off)**，并将其编码为 embedding，为 DiT 提供关于频谱能量分布的线索。

---

### **第二部分：针对语音超分的适应性改造与创新**

这是我们研发工作的核心。我们将在 SAGA-SR 的基础上进行以下关键改造，使其能精准地处理语音信号。

1.  **语义引导的特化：从“音频描述”到“语音转录”**
    *   **目标**: 摒弃模糊的音频描述，采用精确的文字转录作为语义引导，以消除发音歧义、抑制伪影。
    *   **行动**:
        *   **语音转文字 (ASR)**: 使用一个高精度的**自动语音识别 (ASR)** 模型（如 `Whisper`）从输入的低分辨率语音中提取精确的**文字转录**。
        *   **文字转向量**: 沿用 `T5-base` 等预训练文本编码器，将文字转录转换为 embedding。
        *   **注入 DiT**: 将该文本 embedding 通过交叉注意力机制送入 DiT，为语音内容的恢复提供强约束。

2.  **声学引导的增强：引入声纹特征**
    *   **目标**: 在恢复频谱的同时，保持说话人音色的高度一致性。这是纯语音任务的关键需求。
    *   **行动**:
        *   **保留频谱滚降点**: 继续使用 `librosa.feature.spectral_rolloff` 计算频谱滚降点，作为对高频分量丰富度的基本度量。
        *   **新增声纹特征 (Speaker Embedding)**: 使用预训练的声纹识别模型（如 `ECAPA-TDNN` 或 `WavLM`）从低分辨率语音中提取说话人的声学特征向量。
        *   **集成到 DiT**: 将声纹 embedding 与频谱滚降点 embedding、文本 embedding 进行融合（例如，通过拼接后送入交叉注意力，或与时间步 `t` 的 embedding 相加），为 DiT 提供关于“内容”、“音色”和“频谱特性”的全方位引导。

---

### **第三部分：构建完整的训练与推理流程**

1.  **数据准备与预处理**
    *   **目标**: 创建适用于语音超分任务的 (低分辨率语音, 高分辨率语音, 文字转录) 数据对。
    *   **行动**:
        *   **收集数据集**: 采用大规模、多样化的**语音数据集**，如 `LibriTTS`, `VCTK`, `Common Voice` 等，替代原有的通用音频库。
        *   **模拟低分辨率语音**: 对高分辨率语音进行随机的低通滤波。
        *   **预处理**: 将语音切片，并准备好对应的文字转录和声学引导数据（频谱滚降点、声纹特征）。

2.  **训练与推理**
    *   **训练目标**: 沿用 SAGA-SR 的**条件流匹配 (Conditional Flow Matching)** 损失函数进行训练。
    *   **无分类器指导**: 训练时以一定概率丢弃所有条件信息（文本、声学、声纹），以实现推理时的高效引导。
    *   **推理管线**:
        *   输入低分辨率语音 `x_l`。
        *   **预处理**: 运行 ASR 获取转录，同时提取声纹和频谱滚降点。
        *   通过 VAE Encoder 得到 `z_l`。
        *   调用 ODE 采样器，在 DiT 的引导下，从噪声逐步去噪生成 `z_h`。
        *   通过 VAE Decoder 将 `z_h` 转换回高分辨率语音波形。
        *   **后处理**: 将生成语音的低频部分与原始 `x_l` 的低频部分进行融合，确保音调和低频内容的一致性。
