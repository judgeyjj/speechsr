# 语音超分模型开发计划

**目标**：实现一个基于 SAGA-SR 思想的语音超分模型。

**核心组件**:
1.  **VAE**: 使用 `stable-audio-tools` 提供的预训练自编码器。
2.  **DiT**: 使用 `stable-audio-tools` 提供的预置 DiT 架构。
3.  **文本编码器**: 使用 `transformers` 库中的 `Flan-T5` 模型。

---

### 开发阶段

#### Phase 1: 核心组件连接与引导模块构建

*此阶段的目标是打通数据流，将所有独立的模型组件连接成一个整体，并实现完整的引导机制。*

1.  **搭建模型骨架 (`SagaSR_Speech` 类)**
    *   在一个主模型类中，加载预训练的 VAE 和 Flan-T5 模型（设置为冻结/评估模式）。
    *   在同一个类中，根据 `stable-audio-tools` 的配置实例化 DiT 模型（此部分参数可训练）。

2.  **实现引导信息处理器 (Conditioning Processor)**
    *   **语义引导**: 实现一个函数，接收一批文本，通过 Flan-T5 输出 `text_embedding` 张量。
    *   **声学引导**: 实现一个函数，接收一批音频波形，使用 `librosa.feature.spectral_rolloff` 计算**频谱滚降点**，并将其通过傅里叶编码转换为 `rolloff_embedding` 张量。
    *   **信息融合**: 将 `text_embedding` 和 `rolloff_embedding` 整合（例如拼接或相加），准备送入 DiT。

#### Phase 2: 构建数据处理与训练流程

*此阶段的目标是建立一个能为模型持续“喂料”的管道，并让模型能够开始学习。*

1.  **构建数据管道 (`PyTorch Dataset/DataLoader`)**
    *   创建一个数据集类，负责加载音频、生成低分辨率版本，并调用 ASR 获取文字转录。
    *   建立一个数据加载器，将上述数据整理成批次，准备输入模型。

2.  **实现端到端训练逻辑 (`forward` 方法)**
    *   打通从数据输入到损失计算的完整路径：
        1.  数据进入模型。
        2.  调用**引导信息处理器**生成条件张量。
        3.  使用 VAE 将音频转换为潜在表示。
        4.  执行流匹配（Flow Matching）/扩散步骤。
        5.  将所有数据（潜像、时间步、条件张量）送入 DiT 进行预测。
        6.  计算预测结果与目标之间的损失。

3.  **建立训练循环**
    *   编写 `train.py` 脚本，配置优化器（只优化 DiT 的参数），并运行训练循环。

#### Phase 3: 实现推理流程

*此阶段的目标是让训练好的模型能够实际产出结果。*

1.  **构建推理采样器 (Sampler)**
    *   实现一个 ODE 求解器（如欧拉采样器），根据 DiT 的预测逐步去噪。

2.  **封装端到端推理函数**
    *   编写一个函数，输入一个低分辨率音频文件，输出一个高分辨率音频文件。该函数将完整地执行预处理、采样和后处理（VAE 解码）的全部流程。
